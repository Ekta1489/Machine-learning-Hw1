---
title: "Data Science 2,Hw1"
author: "Ekta Chaudhary"
date: "24/02/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(message = F)
```

```{r}
library(tidyverse)
library(caret)
library(ModelMetrics)
library(glmnet)
```

# Reading the Datasets 

```{r}
train_data = read_csv(file = "./data/solubility_train.csv")
test_data = read_csv(file = "./data/solubility_test.csv")
```

We will predict solubility of compounds using their chemical structures.The  training  data  are  in  the  file  “solubilitytrain.csv”  and  the  test  data  are  in  “solubil-itytest.csv”.  Among the 228 predictors, 208 are binary variables that indicate the presenceor absence of a particular chemical substructure, 16 are count features, such as the numberof bonds or the number of bromine atoms, and 4 are continuous features, such as molecularweight or surface area.  The response is in the column “Solubility”.

# Linear Model

```{r}
fit_lm <- lm(Solubility~., data = train_data)
pred_lm <- predict(fit_lm, test_data)
mse(test_data$Solubility, pred_lm)
```

```{r}
x <- model.matrix(Solubility~.,test_data)[,-1]
# vector of response
y <- test_data$Solubility
ridge.mod <- glmnet(x,y, standardize = TRUE,
                    alpha = 0, 
                    lambda = exp(seq(-1, 10, length = 100)))
```

```{r}
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

set.seed(2)
ridge.fit <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0, 
                                          lambda = exp(seq(-1, 10, length = 100))),
                   # preProc = c("center", "scale"),
                   trControl = ctrl1)

plot(ridge.fit, xTrans = function(x) log(x))

ridge.fit$bestTune

coef(ridge.fit$finalModel,ridge.fit$bestTune$lambda)
```

```{r}
set.seed(2)
lasso.fit <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(-1, 5, length = 100))),
                   # preProc = c("center", "scale"),
                   trControl = ctrl1)
plot(lasso.fit, xTrans = function(x) log(x))

lasso.fit$bestTune

coef(lasso.fit$finalModel,lasso.fit$bestTune$lambda)
```

```{r}
set.seed(2)
enet.fit <- train(x, y,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 5), 
                                         lambda = exp(seq(-2, 4, length = 50))),
                  # preProc = c("center", "scale"),
                  trControl = ctrl1)
enet.fit$bestTune

ggplot(enet.fit, highlight = TRUE)
```


```{r, fig.width=5}
set.seed(2)
lm.fit <- train(x, y,
                method = "lm",
                trControl = ctrl1)

resamp <- resamples(list(enet = enet.fit, lasso = lasso.fit, ridge = ridge.fit, lm = lm.fit))
summary(resamp)

parallelplot(resamp, metric = "RMSE")
bwplot(resamp, metric = "RMSE")
```